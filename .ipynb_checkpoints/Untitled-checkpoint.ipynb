{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "# coding=utf-8\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "# stopWords = stopwords.words('english')\n",
    "stopWords = [\",\", \"?\", \"、\", \"。\", \"“\", \"”\", \"《\", \"》\", \"！\", \"，\", \"：\", \"；\", \"？\",\n",
    "\"的\",\"了\",\"在\",\"是\",\"我\",\"有\",\"和\",\"就\",\"不\",\"人\",\"都\",\"一\",\"一个\",\"上\",\"也\",\"很\",\"到\",\"说\",\"要\",\"去\",\"你\",\"会\",\"着\",\"没有\",\"看\",\"好\",\"自己\",\"这\"]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = stopWords)\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "print \"reading topics from 11/19\"\n",
    "# with open('topic_list-11-21.txt') as f:\n",
    "  # content = f.readlines()\n",
    "  # for topic in content:\n",
    "print \"\\n\"\n",
    "# print topic.rstrip()\n",
    "train_set = []\n",
    "# with open('data/'+topic.rstrip()+'.txt') as data:\n",
    "print \"reading topics from 05/16\"\n",
    "with open('topic_list-5-16.txt') as f:\n",
    "  content = f.readlines()\n",
    "  for topic in content:\n",
    "    print \"\\n话题:\"\n",
    "    print topic.rstrip()\n",
    "    train_set = []\n",
    "    with open('weiboData/'+topic.rstrip()+'.txt') as data:\n",
    "    # with open('data/'+'#Blackfish'+'.txt') as data:\n",
    "\t  for tweet in data.readlines():\n",
    "\t    train_set.append(tweet)\n",
    "    trainVectorizerArray = vectorizer.fit_transform(train_set).toarray()\n",
    "    transformer.fit(trainVectorizerArray)\n",
    "    print \"transformer:\"\n",
    "    print trainVectorizerArray\n",
    "    print transformer\n",
    "    sums = transformer.transform(trainVectorizerArray).toarray().sum(1)\n",
    "    sorted_indices = np.argsort(sums)\n",
    "    print \"best\"\n",
    "    print sums[sorted_indices[-1]]\n",
    "    print train_set[sorted_indices[-1]]\n",
    "    print \"second best\"\n",
    "    print sums[sorted_indices[-2]]\n",
    "    print train_set[sorted_indices[-2]]\n",
    "    seenWords = stopWords + train_set[sorted_indices[-1]].split(' ')\n",
    "    # print seenWords\n",
    "    vectorizer2 = CountVectorizer(stop_words = seenWords)\n",
    "    trainVectorizerArray = vectorizer2.fit_transform(train_set).toarray()\n",
    "    transformer2 = TfidfTransformer()\n",
    "    transformer2.fit(trainVectorizerArray)\n",
    "    sums = transformer2.transform(trainVectorizerArray).toarray().sum(1)\n",
    "    sorted_indices = np.argsort(sums)\n",
    "    print \"second best with recompiling\"\n",
    "    print sums[sorted_indices[-1]]\n",
    "    print train_set[sorted_indices[-1]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
